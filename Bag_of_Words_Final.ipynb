{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpKkhr1xA7c9"
   },
   "source": [
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "23HsvxrY4yWI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # dataframe/data cleaning/manipulation\n",
    "import numpy as np # array computations\n",
    "from matplotlib import pyplot as plt # plotting/graphing\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.tree import plot_tree, export_text, DecisionTreeClassifier # Decision tree algorithm and plotting functions for the Decision tree\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes algorithm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score # train test split and cross validation accuracy functions\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score # Various model evaluation metrics\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2 # filter method functions\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS, RFE # wrapper method function\n",
    "from sklearn.feature_extraction.text import CountVectorizer # text data conversion function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2kY-8VuDL9o"
   },
   "source": [
    "Note: If you are using Google Colab, you must upload the `movie.csv` file from Canvas by doing the following:\n",
    "\n",
    "* On the left-side bar, click the folder icon.\n",
    "* Click the 'Upload to session storage' button.\n",
    "* Upload the CSV file; it will appear below the 'sample_data' folder.\n",
    "\n",
    "**Unfortunately, this process must be done every time the runtime is disconnected - just a quirk with Google Colab.**\n",
    "\n",
    "If you are using Jupyter notebook, just make sure the CSV file is in the same folder location as this .ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3z8b5UXWBD8H"
   },
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv('movie.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFtYfh3RBBQY"
   },
   "source": [
    "# Text Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCwbQ513Ec2R"
   },
   "source": [
    "The dataset we are working with is a list of 2000 reviews made for a particular movie, and we are trying to predict (based on the words in each review), if the overall sentiment for each review is positive or negative. The first 5 rows of the dataset are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1704201879233,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "nKC-j7iyEgXh",
    "outputId": "b2bf17f5-fd20-468a-8a01-266846ab4110"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>@@class@@</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text @@class@@\n",
       "0  plot : two teen couples go to a church party ,...       neg\n",
       "1  the happy bastard's quick movie review \\ndamn ...       neg\n",
       "2  it is movies like these that make a jaded movi...       neg\n",
       "3   \" quest for camelot \" is warner bros . ' firs...       neg\n",
       "4  synopsis : a mentally unstable man undergoing ...       neg"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVxa5epyFW-l"
   },
   "source": [
    "Before going further, it's clear we need to do some data manipulation in order to be able to convert this unstructured text into a dataset suitable for training a predictive model.\n",
    "\n",
    "While we already have assigned an outcome/dependent variable to each review in the dataset (whether the review was positive or negative), we need to generate a set of independent variables that encapsulate the essence of the story.\n",
    "\n",
    "To do this, we will use the **\"bag of words\"** methodology, where we take each word from every review as a feature/independent variable in order to represent the contents of the movie reviews holistically. The result will be a structured `(X)` dataset with a massive amount of columns but the same number of rows, where each row represents a movie review, and each column is one of the many words that appeared in the movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmBfeifMI97v"
   },
   "source": [
    "First, let's get a list of each word from every movie review and store it as `text_data`. We can achieve this by using the `.to(list)` method from Pandas on the text column from `movie_df`. The result will be a nested list, where each list within the main list has all the words from a movie review.\n",
    "\n",
    "**Note:** `text_data` will be too large to preview if we call the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qNNhqR3jBICl"
   },
   "outputs": [],
   "source": [
    "text_data = movie_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdUjR029KCNw"
   },
   "source": [
    "Now, in order to create a structured binary dataset that we can use for training, we need to use the [CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function from Scikit-Learn, which will convert our list of words from every movie review into a Numpy matrix of token counts.\n",
    "\n",
    "There are two hyperparameters of interest we will set:\n",
    "\n",
    "- `binary = True` - This will tell the count vectorizer to check whether a particular word (token) is present in the document (1 if present, 0 if not) instead of counting the frequency of each word.\n",
    "- `max_features = 1000` - This will limit the number of features (unique words) to the top 1000, ranked by how often each unique word shows up across the entire list of words.\n",
    "\n",
    "Once we've configured the settings for our count vectorizer, we simply call a `.fit()` on our text data to have the count vectorizer analyze the data to build a vocabulary (bag) of words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zQuJx0qrJd84"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary = True, max_features = 1000).fit(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TupLnh6SNbQt"
   },
   "source": [
    "Like any model object in Scikit-Learn, printing `vectorizer` will only return the settings used for the model. The code below [transforms](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.transform.html) our `text_data` into our desired bag-of-words model, where each nested list is transformed into a vector.\n",
    "\n",
    "**Note:** `bag_of_words` will also be too large to preview if we call the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PrxkpD1eNbow"
   },
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.transform(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXJAd2O3Qrvs"
   },
   "source": [
    "Each vector in `bag_of_words` has a length of 1000 due to `max_features = 1000`, and each element in the vector corresponds to a word in the vocabulary. The elements in the vector are binary (1 or 0) indicating the presence or absence of the word in each movie review. This is better visualized with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1704201895642,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "Efem0bUMOCHD",
    "outputId": "aecc4780-ea5a-4171-ce4d-7fc009689ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 1 0 1]\n",
      " [0 0 0 ... 1 0 1]\n",
      " [0 0 0 ... 1 1 0]\n",
      " ...\n",
      " [1 1 0 ... 1 1 1]\n",
      " [1 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 1]]\n",
      "\n",
      " Our bag of words object has 2000 rows, each representing a movie review, and 1000 columns.\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words.toarray())\n",
    "print('\\n Our bag of words object has', bag_of_words.shape[0], 'rows, each representing a movie review, and', bag_of_words.shape[1], 'columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-7fKyeESkgQ"
   },
   "source": [
    "We can also see the 1000 words selected by our count vectorizer with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1704201895643,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "DEEtLpEfOTh7",
    "outputId": "7d86d35c-bf0d-4bdc-de6e-89d830492ac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', 'ability', 'able', 'about', 'above', 'absolutely', 'across',\n",
       "       'act', 'acting', 'action', 'actor', 'actors', 'actress', 'actual',\n",
       "       'actually', 'add', 'after', 'again', 'against', 'age', 'agent',\n",
       "       'ago', 'air', 'alien', 'all', 'almost', 'alone', 'along',\n",
       "       'already', 'also', 'although', 'always', 'am', 'amazing',\n",
       "       'america', 'american', 'among', 'amount', 'amusing', 'an', 'and',\n",
       "       'annoying', 'another', 'any', 'anyone', 'anything', 'anyway',\n",
       "       'apparently', 'appear', 'appearance', 'appears', 'are', 'aren',\n",
       "       'around', 'art', 'as', 'ask', 'asks', 'aspect', 'at', 'atmosphere',\n",
       "       'attempt', 'attempts', 'attention', 'audience', 'audiences',\n",
       "       'away', 'awful', 'back', 'background', 'bad', 'based', 'basically',\n",
       "       'battle', 'be', 'beautiful', 'because', 'become', 'becomes',\n",
       "       'been', 'before', 'begin', 'beginning', 'begins', 'behind',\n",
       "       'being', 'believable', 'believe', 'best', 'better', 'between',\n",
       "       'beyond', 'big', 'biggest', 'bill', 'bit', 'black', 'blood',\n",
       "       'body', 'book', 'boring', 'both', 'box', 'boy', 'break', 'brief',\n",
       "       'brilliant', 'bring', 'brings', 'brother', 'brothers', 'brought',\n",
       "       'budget', 'bunch', 'business', 'but', 'by', 'call', 'called',\n",
       "       'came', 'camera', 'can', 'cannot', 'car', 'care', 'career', 'case',\n",
       "       'cast', 'casting', 'caught', 'certain', 'certainly', 'chance',\n",
       "       'change', 'character', 'characters', 'chase', 'cheap', 'child',\n",
       "       'children', 'chris', 'cinema', 'cinematic', 'cinematography',\n",
       "       'city', 'class', 'classic', 'clear', 'clearly', 'clever', 'close',\n",
       "       'co', 'cold', 'come', 'comedy', 'comes', 'comic', 'coming',\n",
       "       'common', 'company', 'complete', 'completely', 'complex',\n",
       "       'computer', 'conclusion', 'constantly', 'control', 'convincing',\n",
       "       'cool', 'cop', 'could', 'couldn', 'country', 'couple', 'course',\n",
       "       'create', 'created', 'credit', 'credits', 'crew', 'crime', 'cut',\n",
       "       'dark', 'daughter', 'david', 'day', 'days', 'de', 'dead', 'deal',\n",
       "       'death', 'decent', 'decide', 'decides', 'deep', 'definitely',\n",
       "       'delivers', 'depth', 'despite', 'dialogue', 'did', 'didn', 'die',\n",
       "       'different', 'difficult', 'directed', 'direction', 'director',\n",
       "       'do', 'does', 'doesn', 'dog', 'doing', 'don', 'done', 'doubt',\n",
       "       'down', 'dr', 'drama', 'dramatic', 'dream', 'due', 'dull',\n",
       "       'during', 'each', 'earlier', 'early', 'earth', 'easily', 'easy',\n",
       "       'effect', 'effective', 'effects', 'effort', 'either', 'element',\n",
       "       'elements', 'else', 'emotional', 'end', 'ending', 'ends',\n",
       "       'english', 'enjoy', 'enjoyable', 'enough', 'entertaining',\n",
       "       'entertainment', 'entire', 'entirely', 'escape', 'especially',\n",
       "       'even', 'events', 'eventually', 'ever', 'every', 'everyone',\n",
       "       'everything', 'evil', 'ex', 'exactly', 'example', 'excellent',\n",
       "       'except', 'exciting', 'expect', 'expected', 'experience',\n",
       "       'extremely', 'eye', 'eyes', 'face', 'fact', 'fails', 'fairly',\n",
       "       'fall', 'falls', 'familiar', 'family', 'famous', 'fan', 'fans',\n",
       "       'far', 'fast', 'father', 'favorite', 'fear', 'feature', 'features',\n",
       "       'feel', 'feeling', 'feels', 'felt', 'female', 'few', 'fiction',\n",
       "       'fight', 'figure', 'filled', 'film', 'filmmakers', 'films',\n",
       "       'final', 'finally', 'find', 'finds', 'fine', 'fire', 'first',\n",
       "       'five', 'flat', 'flick', 'focus', 'follow', 'following', 'follows',\n",
       "       'for', 'force', 'forced', 'form', 'former', 'found', 'four',\n",
       "       'free', 'friend', 'friends', 'from', 'front', 'full', 'fun',\n",
       "       'funny', 'further', 'future', 'game', 'gave', 'general', 'genre',\n",
       "       'george', 'get', 'gets', 'getting', 'girl', 'girlfriend', 'give',\n",
       "       'given', 'gives', 'giving', 'go', 'god', 'goes', 'going', 'gone',\n",
       "       'good', 'got', 'great', 'greatest', 'group', 'guess', 'gun', 'guy',\n",
       "       'guys', 'had', 'half', 'hand', 'hands', 'happen', 'happened',\n",
       "       'happens', 'happy', 'hard', 'has', 'have', 'haven', 'having', 'he',\n",
       "       'head', 'hear', 'heard', 'heart', 'heavy', 'hell', 'help', 'her',\n",
       "       'here', 'hero', 'herself', 'high', 'highly', 'hilarious', 'him',\n",
       "       'himself', 'his', 'history', 'hit', 'hold', 'hollywood', 'home',\n",
       "       'hope', 'horror', 'hot', 'hour', 'hours', 'house', 'how',\n",
       "       'however', 'huge', 'human', 'humor', 'husband', 'idea', 'ideas',\n",
       "       'if', 'imagine', 'immediately', 'important', 'impossible',\n",
       "       'impressive', 'in', 'including', 'indeed', 'inside', 'instead',\n",
       "       'intelligent', 'interest', 'interesting', 'into', 'involved',\n",
       "       'involving', 'is', 'isn', 'it', 'its', 'itself', 'jack', 'james',\n",
       "       'jim', 'job', 'joe', 'john', 'joke', 'jokes', 'just', 'keep',\n",
       "       'keeps', 'kevin', 'key', 'kid', 'kids', 'kill', 'killed', 'killer',\n",
       "       'kind', 'king', 'know', 'known', 'knows', 'lack', 'large', 'last',\n",
       "       'late', 'later', 'latest', 'laugh', 'laughs', 'law', 'lead',\n",
       "       'leads', 'learn', 'least', 'leave', 'leaves', 'leaving', 'lee',\n",
       "       'left', 'less', 'let', 'level', 'life', 'light', 'like', 'liked',\n",
       "       'likely', 'line', 'lines', 'list', 'little', 'live', 'lives',\n",
       "       'living', 'll', 'local', 'long', 'longer', 'look', 'looking',\n",
       "       'looks', 'lost', 'lot', 'lots', 'love', 'low', 'made', 'main',\n",
       "       'major', 'make', 'makes', 'making', 'man', 'manages', 'many',\n",
       "       'mark', 'married', 'martin', 'material', 'matter', 'may', 'maybe',\n",
       "       'me', 'mean', 'means', 'meanwhile', 'meet', 'meets', 'member',\n",
       "       'members', 'memorable', 'men', 'merely', 'mess', 'message',\n",
       "       'michael', 'middle', 'might', 'million', 'mind', 'minor', 'minute',\n",
       "       'minutes', 'mission', 'modern', 'moment', 'moments', 'money',\n",
       "       'more', 'most', 'mostly', 'mother', 'motion', 'move', 'moves',\n",
       "       'movie', 'movies', 'moving', 'mr', 'much', 'murder', 'music',\n",
       "       'must', 'my', 'myself', 'mystery', 'name', 'named', 'nature',\n",
       "       'near', 'nearly', 'need', 'needs', 'neither', 'never', 'new',\n",
       "       'next', 'nice', 'night', 'no', 'non', 'none', 'nor', 'not', 'note',\n",
       "       'nothing', 'novel', 'now', 'number', 'obvious', 'obviously', 'of',\n",
       "       'off', 'offers', 'office', 'often', 'oh', 'old', 'on', 'once',\n",
       "       'one', 'ones', 'only', 'open', 'opening', 'opens', 'opportunity',\n",
       "       'or', 'order', 'original', 'oscar', 'other', 'others', 'otherwise',\n",
       "       'our', 'out', 'outside', 'over', 'overall', 'own', 'parents',\n",
       "       'part', 'particular', 'particularly', 'parts', 'past', 'paul',\n",
       "       'pay', 'people', 'perfect', 'perfectly', 'performance',\n",
       "       'performances', 'perhaps', 'person', 'personal', 'peter',\n",
       "       'picture', 'piece', 'place', 'plan', 'planet', 'play', 'played',\n",
       "       'playing', 'plays', 'plenty', 'plot', 'point', 'points', 'police',\n",
       "       'poor', 'popular', 'possible', 'possibly', 'potential', 'power',\n",
       "       'powerful', 'predictable', 'premise', 'presence', 'present',\n",
       "       'pretty', 'previous', 'probably', 'problem', 'problems',\n",
       "       'produced', 'production', 'project', 'proves', 'provide',\n",
       "       'purpose', 'put', 'puts', 'quality', 'question', 'questions',\n",
       "       'quick', 'quickly', 'quite', 'rated', 'rather', 'rating', 're',\n",
       "       'read', 'real', 'reality', 'realize', 'really', 'reason',\n",
       "       'reasons', 'recent', 'recently', 'red', 'relationship', 'release',\n",
       "       'released', 'remains', 'remember', 'rest', 'result', 'return',\n",
       "       'review', 'rich', 'ride', 'ridiculous', 'right', 'robert', 'robin',\n",
       "       'rock', 'role', 'roles', 'romance', 'romantic', 'room', 'run',\n",
       "       'running', 'runs', 'said', 'same', 'save', 'saw', 'say', 'saying',\n",
       "       'says', 'scene', 'scenes', 'school', 'science', 'score', 'scott',\n",
       "       'screen', 'screenplay', 'screenwriter', 'script', 'second',\n",
       "       'secret', 'see', 'seeing', 'seem', 'seemed', 'seemingly', 'seems',\n",
       "       'seen', 'sees', 'self', 'sense', 'sequel', 'sequence', 'sequences',\n",
       "       'series', 'serious', 'seriously', 'set', 'sets', 'setting',\n",
       "       'several', 'sex', 'sexual', 'she', 'short', 'shot', 'shots',\n",
       "       'should', 'show', 'showing', 'shown', 'shows', 'side', 'silly',\n",
       "       'similar', 'simple', 'simply', 'since', 'single', 'sit',\n",
       "       'situation', 'situations', 'six', 'slightly', 'slow', 'slowly',\n",
       "       'small', 'smart', 'so', 'society', 'solid', 'some', 'somehow',\n",
       "       'someone', 'something', 'sometimes', 'somewhat', 'somewhere',\n",
       "       'son', 'soon', 'sort', 'sound', 'sounds', 'soundtrack', 'space',\n",
       "       'special', 'spend', 'spent', 'stand', 'star', 'starring', 'stars',\n",
       "       'start', 'starts', 'state', 'stay', 'still', 'stop', 'stories',\n",
       "       'story', 'straight', 'strange', 'street', 'strong', 'stuff',\n",
       "       'stupid', 'style', 'subject', 'subtle', 'success', 'successful',\n",
       "       'such', 'suddenly', 'summer', 'supporting', 'supposed', 'sure',\n",
       "       'surprise', 'surprisingly', 'suspense', 'sweet', 'take', 'taken',\n",
       "       'takes', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking',\n",
       "       'team', 'television', 'tell', 'telling', 'tells', 'ten', 'tension',\n",
       "       'terrible', 'than', 'that', 'the', 'theater', 'their', 'them',\n",
       "       'themselves', 'then', 'there', 'these', 'they', 'thing', 'things',\n",
       "       'think', 'thinking', 'thinks', 'third', 'this', 'those', 'though',\n",
       "       'thought', 'three', 'thriller', 'through', 'throughout', 'time',\n",
       "       'times', 'title', 'to', 'today', 'together', 'told', 'tom', 'tone',\n",
       "       'too', 'took', 'top', 'totally', 'towards', 'town', 'tries',\n",
       "       'trouble', 'true', 'truly', 'truth', 'try', 'trying', 'turn',\n",
       "       'turned', 'turns', 'tv', 'two', 'type', 'typical', 'ultimately',\n",
       "       'under', 'understand', 'unfortunately', 'unlike', 'until', 'up',\n",
       "       'upon', 'us', 'use', 'used', 'uses', 'using', 'usual', 'usually',\n",
       "       'various', 've', 'version', 'very', 'video', 'view', 'viewer',\n",
       "       'viewers', 'villain', 'violence', 'violent', 'visual', 'voice',\n",
       "       'wait', 'want', 'wanted', 'wants', 'war', 'was', 'wasn', 'waste',\n",
       "       'watch', 'watching', 'water', 'way', 'ways', 'we', 'well', 'went',\n",
       "       'were', 'what', 'whatever', 'when', 'where', 'whether', 'which',\n",
       "       'while', 'white', 'who', 'whole', 'whom', 'whose', 'why', 'wife',\n",
       "       'wild', 'will', 'william', 'williams', 'with', 'within', 'without',\n",
       "       'woman', 'women', 'won', 'wonder', 'wonderful', 'word', 'words',\n",
       "       'work', 'working', 'works', 'world', 'worse', 'worst', 'worth',\n",
       "       'would', 'wouldn', 'writer', 'writing', 'written', 'wrong',\n",
       "       'wrote', 'year', 'years', 'yes', 'yet', 'york', 'you', 'young',\n",
       "       'your'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYSPvLhmTHoY"
   },
   "source": [
    "# Create and Evaluate Model - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-pK3C5_U4xN"
   },
   "source": [
    "Now that we have created a structured `X` dataset for training, we are ready to construct a classification model to start making predictions! To begin, we can define our features and target variable from our dataset and split them into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "z8lPYOOUVgqe"
   },
   "outputs": [],
   "source": [
    "X = bag_of_words\n",
    "y = movie_df.iloc[:,1] # class variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40f-mI63V5rF"
   },
   "source": [
    "To start, let's create a decision tree model using the settings we have been using previously for the British Bank Dataset and evaluate the model based on 10-Fold Cross Validation ROC AUC % and Test Classification Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2159,
     "status": "ok",
     "timestamp": 1704201897799,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "u1UlYRf8WZgE",
    "outputId": "6d35d4d0-044c-40b6-b6fc-011d566044b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 10-Fold Cross Validation ROC AUC: 67.44 %\n",
      "Decision Tree Test Classification Accuracy: 60.15 %\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, random_state = 3).fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'roc_auc').mean()\n",
    "print('Decision Tree 10-Fold Cross Validation ROC AUC:', round(score*100,2),'%')\n",
    "\n",
    "print('Decision Tree Test Classification Accuracy:', round(float(accuracy_score(model.predict(X_test), y_test)) * 100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23Ka7EUPXO1T"
   },
   "source": [
    "The above results are definitely less-than-desireable; decision trees are typically not the best choice for modeling text data, especially when represented as a binary bag-of-words. There are a few reasons why:\n",
    "\n",
    "1. **High Dimensionality:** Text data, especially when transformed into a bag-of-words model, tends to be high-dimensional (i.e., it has many features). Even with a limit of 1000 words, each data point is represented as a vector of 1000 dimensions. Decision trees are not very efficient with high-dimensional data and can lead to overfitting.\n",
    "\n",
    "2. **Sparse Data:** Bag-of-words models are typically sparse, meaning that most of the features (words) will have a value of 0 (absent) for most documents. Decision trees, which rely on partitioning data based on feature values, can struggle with sparse data. They might end up creating very complex trees that do not generalize well.\n",
    "\n",
    "3. **Binary Features**: In our specific case, the bag-of-words model is binary. This can limit the effectiveness of decision trees, as they just split on whether a word is present or not, without considering the frequency or weight of the words.\n",
    "\n",
    "4. **Computational Complexity:** For large datasets, decision trees can become computationally expensive to train. The complexity increases with the number of features and depth of the tree. With high-dimensional data, trees can grow deep and large, leading to increased computational costs and potentially poorer performance.\n",
    "\n",
    "Moreover, while decision trees can show how words interact (using \"if\" statements), but they might get too fixated on the specific stories theyâ€™ve seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNVbi5Q6Y6Rp"
   },
   "source": [
    "Fortunately, as discussed in class, there is another model type that is able to handle text data more effectively, and is discussed next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6V7Kg44wUdOv"
   },
   "source": [
    "# Create and Evaluate Model - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEn45GxOZHCf"
   },
   "source": [
    "The Naive Bayes classifier is a simple yet powerful tool for text classification. The basic principle, in context, is that if a movie review contains words commonly seen in negative reviews and rarely in positive reviews, the review is more likely to be negative (and vice versa).\n",
    "\n",
    "Essentially, Naive Bayes helps us decide whether a review is likely positive or negative based on the words it contains and conditional probabilities.\n",
    "\n",
    "**What makes the method \"Naive\" is that the classifier assumes that the presence of each word is independent of the others.** As a result, Naive Bayes does not capture the way some words work together to change the overall meaning of a statement, which can be important (especially if we don't have many words to work with).\n",
    "\n",
    "However, the naive assumption simplifies the required calculations and generally produces much better results for large text data than any of the other classification models covered in this class!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB0G5yoBeKDE"
   },
   "source": [
    "In Scikit-Learn, we use the [MultinomialNB()](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) to fit and train a Naive Bayes model!\n",
    "\n",
    "There are very few parameters that can be set for this Naive Bayes compared to other models and that are in the scope of discussing in this class, but the two parameters you can experiment with are:\n",
    "\n",
    "- `alpha` - Additive smoothing value.\n",
    "- `force_alpha` - If set to false and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1704201898260,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "oVV0ZKUVZHY6",
    "outputId": "ee817959-de27-441e-9f6b-cdee1be67d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 10-Fold Cross Validation ROC AUC: 89.31 %\n",
      "Naive Bayes Test Classification Accuracy: 79.85 %\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'roc_auc').mean()\n",
    "print('Naive Bayes 10-Fold Cross Validation ROC AUC:', round(score*100,2),'%')\n",
    "\n",
    "print('Naive Bayes Test Classification Accuracy:', round(float(accuracy_score(model.predict(X_test), y_test)) * 100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iND_L4o_f-EV"
   },
   "source": [
    "As seen above, our Naive Bayes model performed much better than our Decision Tree model. Try experiment with Naive Bayes to see if you can get better results than the variations shown above!\n",
    "\n",
    "Alternatively, another model in Scikit-Learn that you could look into for text data is [Support Vector Machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) or (Convolutional) Neural Networks through Tensorflow/Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6tjwroJUguU"
   },
   "source": [
    "# Applying Feature Selection to Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp431vz1hC0w"
   },
   "source": [
    "We can apply the feature discussion techniques discussed in class and in the Feature_Selection_Final.ipynb file on Canvas to Naive Bayes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQoPWuNWiiwn"
   },
   "source": [
    "## Filter Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMKWo9phh7ZS"
   },
   "source": [
    "Suppose we want to limit our selection of words further from the 1000 most frequent words we originally selected when converting the unstructured text into a dataset suitable for training a predictive model to the top 250 words.\n",
    "\n",
    "Using the `SelectKBest()` function to to automatically select the top 250 words from our `X` dataset, we can compare the results between scoring based on mutual information (through `mutual_info_classif`) and the chi-square test (through `chi2`).\n",
    "\n",
    "**Note:** `fit_transform` is a method that allows you to use the `fit()` and `transform()` functions at the same time. In this case, we use it to calculate the score for each feature (words) in 'X' with respect to our target variable class ('y') AND based on those scores, only select the subset of 250 words from ('X') that had the highest scores based on mutual information or the chi-square test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7867,
     "status": "ok",
     "timestamp": 1704201906125,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "j46PsOUekW3x",
    "outputId": "409902c2-6234-4837-b155-7afc31af7df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Naive Bayes 10-Fold Cross Validation ROC AUC: 91.83 %\n",
      "Mutual Information Naive Bayes Test Classification Accuracy: 83.64 %\n"
     ]
    }
   ],
   "source": [
    "# Mutual Information\n",
    "\n",
    "X_new = SelectKBest(mutual_info_classif, k = 250).fit_transform(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.33)\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'roc_auc').mean()\n",
    "print('Mutual Information Naive Bayes 10-Fold Cross Validation ROC AUC:', round(score*100,2),'%')\n",
    "\n",
    "print('Mutual Information Naive Bayes Test Classification Accuracy:', round(float(accuracy_score(model.predict(X_test), y_test)) * 100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1704201906359,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "BTAxhX1-krm0",
    "outputId": "4c499638-c724-4d59-d77f-b53db1322a88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Naive Bayes 10-Fold Cross Validation ROC AUC: 91.46 %\n",
      "Chi-Square Naive Bayes Test Classification Accuracy: 85.91 %\n"
     ]
    }
   ],
   "source": [
    "# Chi-Square Test\n",
    "\n",
    "X_new = SelectKBest(chi2, k = 250).fit_transform(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.33)\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'roc_auc').mean()\n",
    "print('Chi-Square Naive Bayes 10-Fold Cross Validation ROC AUC:', round(score*100,2),'%')\n",
    "\n",
    "print('Chi-Square Naive Bayes Test Classification Accuracy:', round(float(accuracy_score(model.predict(X_test), y_test)) * 100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aO-nCKhilDj"
   },
   "source": [
    "## Wrapper Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldZGLtkUimwL"
   },
   "source": [
    "We can apply the wrapper method using the `SFS()` function and use forward selection to find the best subset of 250 words to use from the 1000 most frequent words we originaly selected when converting the unstructured text into a dataset suitable for training a predictive model.\n",
    "\n",
    "**Note:** This will take a SIGNIFICANT amount of time to run **(we strongly recommend not running this on Google Colab)**, as unlike any of the other datasets we have worked with previously, we have 1000 different features to consider as opposed to ~ 54 from the employee attrition dataset and ~ 13 from the british bank dataset. The `n_jobs = -1` parameter tells SFS to use all processors on the machine to parallelize and speed up the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OCHRMFudq5Xt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper-Selected Naive Bayes 10-Fold Cross Validation ROC AUC: 93.89 %\n",
      "Wrapper-Selected Naive Bayes Test Classification Accuracy: 86.36 %\n"
     ]
    }
   ],
   "source": [
    "sfs = SFS(model, n_features_to_select=250, direction='forward', scoring='roc_auc', cv=10, n_jobs = -1).fit(X,y)\n",
    "\n",
    "X_new = sfs.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.33)\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'roc_auc').mean()\n",
    "print('Wrapper-Selected Naive Bayes 10-Fold Cross Validation ROC AUC:', round(score*100,2),'%')\n",
    "\n",
    "print('Wrapper-Selected Naive Bayes Test Classification Accuracy:', round(float(accuracy_score(model.predict(X_test), y_test)) * 100,2),'%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyttcOyv8EQVMelUtTxs8W",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
