{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlFv59qFzRCG"
   },
   "source": [
    "# Import Libraries/Data and Instantiating a Sample Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1703820087519,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "mbbQMceauW_-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # dataframe/data cleaning/manipulation\n",
    "import numpy as np # array computations\n",
    "from matplotlib import pyplot as plt # plotting/graphing\n",
    "import seaborn as sns # additional visualizations\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision tree algorithm\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score # classification, confusion matrix, and ROC AUC accuracy metrics along with display functions\n",
    "from sklearn.model_selection import train_test_split, cross_val_score # train test split and cross validation accuracy function\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif # filter method functions\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS, RFE # wrapper method functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q_3H7My0HW3"
   },
   "source": [
    "Note: If you are using Google Colab, you must upload the training and testing CSVs from Canvas by doing the following:\n",
    "\n",
    "* On the left-side bar, click the folder icon.\n",
    "* Click the 'Upload to session storage' button.\n",
    "* Upload the two CSV files; they will appear below the 'sample_data' folder.\n",
    "\n",
    "**Unfortunately, this process must be done every time the runtime is disconnected - just a quirk with Google Colab.**\n",
    "\n",
    "If you are using Jupyter notebook, just make sure the training and testings CSV files are in the same folder location as this .ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1703820088645,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "PsW_wT2N0H6S"
   },
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('training_data.csv',index_col=0)\n",
    "testing_df = pd.read_csv('testing_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsuAh1bu1xZQ"
   },
   "source": [
    "As before, we are working with the British Bank Dataset with ~ 600 records aiming to predict whether or not someone will buy a personal equity plan (PEP) based on other data such as age, sex, region, and income.\n",
    "\n",
    "Our main goal is to explore the forms of feature selection discussed in class. As in the previous two Python notebooks, we will instantiate a sample model to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1703820089490,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "JvhFHgKS2M4m"
   },
   "outputs": [],
   "source": [
    "X = training_df.drop(columns = ['pep'])\n",
    "y = training_df.pep\n",
    "model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, random_state = 3).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0RBNiZOzctK"
   },
   "source": [
    "# Decision Tree Pre-Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEKzcAaEXKic"
   },
   "source": [
    " Pre-pruning involves stopping the decision tree before it has finished classifying the training set as a method of preventing overfitting; it is a greedy approach in that we might avoid a split that results in a subsequent split being extremely valuable for the model. However, it is not as computationally expensive as post-pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLx77rUGZP1s"
   },
   "source": [
    "Pre-pruning a decision tree can be tackled a few different ways in Scikit-Learn.\n",
    "\n",
    "**One way is to limit the depth of the tree using the `max_depth` parameter.**\n",
    "\n",
    "To experiment, we will again split the training data further using the `train_test_split()` function as shown in the Model_Evaluation_Final.ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 91,
     "status": "ok",
     "timestamp": 1703820093987,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "xR58WT2fzh5S"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hDnmABgZz_7"
   },
   "source": [
    "From here, we will instantiate a for-loop where the tree's depth will vary from a maximum of 10 to a minimum of 2. The idea is that we are restricting the tree's depth during its initial creation rather than letting it grow fully. Pruning the model after it has been created would be a form of post-pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 839,
     "status": "ok",
     "timestamp": 1703820095851,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "NcczJXu1aNy6",
    "outputId": "1600f0bf-3dbd-4fd7-f05c-8e1453098f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a max_depth value of 10: Test Classification Accuracy: 85.0%, 10-Fold CV Accuracy: 83.0%\n",
      "For a max_depth value of 9: Test Classification Accuracy: 86.0%, 10-Fold CV Accuracy: 83.0%\n",
      "For a max_depth value of 8: Test Classification Accuracy: 89.0%, 10-Fold CV Accuracy: 85.0%\n",
      "For a max_depth value of 7: Test Classification Accuracy: 89.0%, 10-Fold CV Accuracy: 86.0%\n",
      "For a max_depth value of 6: Test Classification Accuracy: 88.0%, 10-Fold CV Accuracy: 87.0%\n",
      "For a max_depth value of 5: Test Classification Accuracy: 83.0%, 10-Fold CV Accuracy: 84.0%\n",
      "For a max_depth value of 4: Test Classification Accuracy: 84.0%, 10-Fold CV Accuracy: 83.0%\n",
      "For a max_depth value of 3: Test Classification Accuracy: 74.0%, 10-Fold CV Accuracy: 74.0%\n",
      "For a max_depth value of 2: Test Classification Accuracy: 57.99999999999999%, 10-Fold CV Accuracy: 59.0%\n"
     ]
    }
   ],
   "source": [
    "for depth in reversed(range(2,11)):\n",
    "   temp_model = DecisionTreeClassifier(criterion = 'entropy', max_depth=depth).fit(X_train, y_train)\n",
    "   print(f\"For a max_depth value of {depth}: Test Classification Accuracy: {round(accuracy_score(y_test, temp_model.predict(X_test)),2)*100}%, 10-Fold CV Accuracy: {round((cross_val_score(temp_model, X, y, cv = 10)).mean(),2)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07NRqFb5cNsP"
   },
   "source": [
    "We can apply this same concept to the `min_samples_leaves` parameter, which will not allow splits that result in leaf nodes with less than the specified number of samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1703820099210,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "0cDKxz2ycpZM",
    "outputId": "e6b5ecbd-f965-443f-f78e-ae72069ad1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a min_samples_leaf value of 10: Test Classification Accuracy: 87.0%, 10-Fold CV Accuracy: 88.0%\n",
      "For a min_samples_leaf value of 9: Test Classification Accuracy: 87.0%, 10-Fold CV Accuracy: 88.0%\n",
      "For a min_samples_leaf value of 8: Test Classification Accuracy: 87.0%, 10-Fold CV Accuracy: 88.0%\n",
      "For a min_samples_leaf value of 7: Test Classification Accuracy: 84.0%, 10-Fold CV Accuracy: 86.0%\n",
      "For a min_samples_leaf value of 6: Test Classification Accuracy: 85.0%, 10-Fold CV Accuracy: 86.0%\n",
      "For a min_samples_leaf value of 5: Test Classification Accuracy: 87.0%, 10-Fold CV Accuracy: 86.0%\n",
      "For a min_samples_leaf value of 4: Test Classification Accuracy: 90.0%, 10-Fold CV Accuracy: 84.0%\n",
      "For a min_samples_leaf value of 3: Test Classification Accuracy: 87.0%, 10-Fold CV Accuracy: 85.0%\n",
      "For a min_samples_leaf value of 2: Test Classification Accuracy: 86.0%, 10-Fold CV Accuracy: 85.0%\n"
     ]
    }
   ],
   "source": [
    "for leaf in reversed(range(2,11)):\n",
    "   temp_model = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf=leaf).fit(X_train, y_train)\n",
    "   print(f\"For a min_samples_leaf value of {leaf}: Test Classification Accuracy: {round(accuracy_score(y_test, temp_model.predict(X_test)),2)*100}%, 10-Fold CV Accuracy: {round((cross_val_score(temp_model, X, y, cv = 10)).mean(),2)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wutzyTPadWcJ"
   },
   "source": [
    "By increasing the minimum number of samples required at a leaf, the tree might become more generalized, reducing the risk of overfitting. On the other hand, setting this value too high might lead to underfitting where the tree does not capture sufficient patterns from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDJKNOmBzl4Q"
   },
   "source": [
    "# Filter Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0XasqLUd1pR"
   },
   "source": [
    "The filter selection method involves selecting features based on their statistical properities and relevance to the target variable (in our case, PEP), independent of any machine learning algorithm. It differs from the aforementioned concept of pre-pruning in that it is a pre-processing step applied before the algorithm is run and can be applied to almost all types of models.\n",
    "\n",
    "There are various statistical measures we can use to \"score\" each feature, including but not limited to:\n",
    "\n",
    "- Correlation coefficients (via a correlation matrix)\n",
    "- Chi-square test\n",
    "- Mutual information\n",
    "\n",
    "Based on these scores, we can then sequentially assess and remove each feature IF the model's performance improves by doing so.\n",
    "\n",
    "The main advantages of the filter method is that it is not computationally expensive and is not tailored to a specific model type. However, it does not necessarily consider interactions between features.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gmoMSTR5pt6"
   },
   "source": [
    "Let's explore one of the aforementioned statistical measures, mutual information, in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y5dH79gziEW"
   },
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ie_c11f51n4"
   },
   "source": [
    "Mutual information is a measure of the mutual dependence between two variables. It quantifies how much information a feature provides about the target variable.\n",
    "\n",
    "For each feature in the dataset, mutual information calculates how much knowing the value of that feature reduces uncertainty about the target variable.\n",
    "\n",
    "A higher value of mutual information indicates that the feature is more informative regarding the target variable. Conversely, a mutual information value close to zero suggests that the feature is less relevant for predicting the target variable.\n",
    "\n",
    "To compute and identify these values in Scikit-Learn, we will utilize the [SelectKBest()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) and the [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GR8riNKDIrl4"
   },
   "source": [
    "`SelectKBest` is able to select the top 'k' features based on a scoring function, which in this case is `mutual_info_classif`. A similar function exists for chi-squared.\n",
    "\n",
    "In this case, we are interested in the mutual information values for all of our features, so we will not utilize the 'k' parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1703822241739,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "btM3ZI55eLOu"
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(mutual_info_classif).fit(X,y)\n",
    "# By calling .fit(X,y), the mutual information score for each feature in 'X' is calcluated with respect to our target variable PEP ('y').\n",
    "scores = selector.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1703822242387,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "fdW9YDwsKQUw",
    "outputId": "e76cd6fa-a78d-4520-8b29-bc8ae1ec56e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00189369 0.         0.08229865 0.         0.02233835\n",
      " 0.03679827 0.         0.         0.01662535 0.01089824 0.01010141\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# If we call the scores object by itself, we don't know which values correspond to which features in X.\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1703822244956,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "RXOSkyGnJTAa",
    "outputId": "a8ef6211-2751-4add-d683-05066fee83a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature age: 0.000000\n",
      "Feature income: 0.001894\n",
      "Feature married: 0.000000\n",
      "Feature children: 0.082299\n",
      "Feature car: 0.000000\n",
      "Feature save_act: 0.022338\n",
      "Feature current_act: 0.036798\n",
      "Feature mortgage: 0.000000\n",
      "Feature sex_FEMALE: 0.000000\n",
      "Feature region_INNER_CITY: 0.016625\n",
      "Feature region_RURAL: 0.010898\n",
      "Feature region_SUBURBAN: 0.010101\n",
      "Feature region_TOWN: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Technically, each value corresponds to each column in the 'X' dataframe from left to right. We can use Python/Pandas to make our results more interpretable.\n",
    "\n",
    "# Get the names of all features\n",
    "feature_names=list(X.columns)\n",
    "\n",
    "# Print the scores for each feature\n",
    "for i, score in enumerate(selector.scores_):\n",
    "    print(\"Feature %s: %f\" % (feature_names[i], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRXGB8kALARO"
   },
   "source": [
    "What takeaways can we make from these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTznjbeRuNez"
   },
   "source": [
    "Note: There will be variation in the results each time you run the code using SelectKBest with mutual_info_classif is due to the inherent randomness within the mutual_info_classif function in scikit-learn. You can define a function to set the random state for the mutual_info_classif function, but the results in the next section should not change regardless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGtRX-PJQZLP"
   },
   "source": [
    "## Applying Mutual Information to the Filter Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_d_739DLFXR"
   },
   "source": [
    "Now that we have the scores for each feature based on mutual information, we can use the filter method to understand the impact of each feature on the model's importance.\n",
    "\n",
    "To start, we can create a score dictionary to map each feature name to its mutual information score and then sort the dictionary in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1703822247291,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "_RBXXhiALi7E",
    "outputId": "bd1e0e9e-bf57-42cf-ae42-4e0eff87ed1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('children', 0.08229865136087411), ('current_act', 0.03679827428447746), ('save_act', 0.022338347637050893), ('region_INNER_CITY', 0.01662534536154836), ('region_RURAL', 0.010898236141792417), ('region_SUBURBAN', 0.010101406337609475), ('income', 0.0018936945902052749), ('age', 0.0), ('married', 0.0), ('car', 0.0), ('mortgage', 0.0), ('sex_FEMALE', 0.0), ('region_TOWN', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the selected features\n",
    "selected_features_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Create a dictionary that maps feature names to their scores\n",
    "score_dict = dict(zip(feature_names, scores))\n",
    "\n",
    "# Sort the dictionary by scores in descending order\n",
    "sorted_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQdrOXpgOTzj"
   },
   "source": [
    "In order to assess whether each feature we remove either improves or deteriorates model performance, we need a baseline metric to compare against for our model. In this case, we will use the 10-Fold Cross Validation ROC AUC % on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1703822248968,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "K-XBgIpPLR6j",
    "outputId": "8d69adc7-b83e-4048-c4c0-6923f3d219fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision-Tree 10-Fold Cross Validation ROC AUC: 86.63 %\n"
     ]
    }
   ],
   "source": [
    "mycv = cross_val_score(model, X, y, scoring='roc_auc', cv = 10).mean()\n",
    "print('Decision-Tree 10-Fold Cross Validation ROC AUC:', round(mycv*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H7zX_BcQSZ7"
   },
   "source": [
    "Now that we have our baseline, we can instantiate a for-loop that iteratively removes each feature from 'X', constructs a new model, and reports the 10-Fold Cross Validation ROC AUC % for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1186,
     "status": "ok",
     "timestamp": 1703822255470,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "_x8oP5h1OIl_",
    "outputId": "dc733dd0-3733-4511-c4e8-8beffc337d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing children : 63.68 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing current_act : 86.13 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing save_act : 87.02 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing region_INNER_CITY : 86.88 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing region_RURAL : 86.95 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing region_SUBURBAN : 87.24 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing income : 84.79 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing age : 87.6 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing married : 83.29 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing car : 86.7 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing mortgage : 86.83 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing sex_FEMALE : 87.29 %\n",
      "Decision-Tree 10-Fold Cross Validation ROC AUC after removing region_TOWN : 86.97 %\n",
      "\n",
      "Features to Remove: ['save_act', 'region_INNER_CITY', 'region_RURAL', 'region_SUBURBAN', 'age', 'car', 'mortgage', 'sex_FEMALE', 'region_TOWN']\n"
     ]
    }
   ],
   "source": [
    "# List to hold features that should be removed.\n",
    "features_to_remove = []\n",
    "\n",
    "for feature, _ in sorted_dict:\n",
    "\n",
    "    # Drop one feature at a time\n",
    "    X_temp = X.drop(columns=[feature])\n",
    "\n",
    "    # Construct a new model with the removed feature\n",
    "    model_temp = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=3)\n",
    "\n",
    "    # Calculate the 10-Fold Cross Validation ROC AUC % on the new model.\n",
    "    auc_score = cross_val_score(model_temp, X_temp, y, scoring='roc_auc', cv=10).mean()\n",
    "\n",
    "    print('Decision-Tree 10-Fold Cross Validation ROC AUC after removing', feature,':', round(auc_score*100, 2), '%')\n",
    "\n",
    "    if auc_score >= mycv:\n",
    "        features_to_remove.append(feature)\n",
    "\n",
    "print('\\nFeatures to Remove:',features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVcqu8gvUkeQ"
   },
   "source": [
    "Based on the results, it appears there are several features we should remove. Do these results surprise you given that last few variables listed were ranked highly in terms of mutual information? How is this possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRZLZWVJW7ku"
   },
   "source": [
    "We can now apply the results from the filter method by removing the specified attributes and see how our model performs in terms of 10-Fold Cross Validation ROC AUC %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1703822257589,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "Il5rd-2FWotr"
   },
   "outputs": [],
   "source": [
    "X_filter = X.drop(columns=features_to_remove, axis=1)\n",
    "model_filter = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5, random_state = 3).fit(X_filter, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1703822258442,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "d2StK9C4XSYb",
    "outputId": "caad3f5e-7ed4-4c1e-f151-bb1e9ddfd44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision-Tree 10-Fold Cross Validation ROC AUC after Filtering: 87.16 %\n"
     ]
    }
   ],
   "source": [
    "mycv_filter = cross_val_score(model_filter, X_filter, y, scoring='roc_auc',cv = 10).mean()\n",
    "print('Decision-Tree 10-Fold Cross Validation ROC AUC after Filtering:', round(mycv_filter*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOlK2LpkzovC"
   },
   "source": [
    "# Wrapper Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_C2n94zeMbs"
   },
   "source": [
    "The wrapper method uses a specific machine learning model to evaluate the effectiveness of subsets of features. The selection of features is \"wrapped\" around this model; and hence the optimal set of features is specific to the model being used.\n",
    "\n",
    "- For example, the features selected for a decision tree model might not be the best for another model, such as a logistic regression.\n",
    "\n",
    "Essentially, the wrapper method involves training a model on different combinations of features and evaluating their performance; aiming to find the best subset of features that result in the best model performance. A search algorithm (like forward selection, backward elimination, or recursive feature elimination) iteratively adds or removes features, evaluates the model, and then decides the next set of features to try. Moreover, the wrapper method uses the performance of a specific machine learning model as the criterion for feature selection, as opposed to general statistical measures in the filter method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCg9JUkjarUP"
   },
   "source": [
    "**The wrapper method is the most computationally costly of all the feature selection methods covered in this class, as they involve training models multiple times for different subsets of features.** This is important to note as you will notice Google Colab will take significantly longer to run the code, especially in the context of HW4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4FJz534cVRj"
   },
   "source": [
    "Firstly, as with the filter method, we will need a baseline metric to compare against for our model. Again, we can use the 10-Fold Cross Validation ROC AUC % on the training data as our metric of interest. This is our inital model performance before we perform the wrapper method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1703823396876,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "qs6sauGOzqaC",
    "outputId": "dc50f85c-f2e4-4884-943d-7a4a97012096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision-Tree 10-Fold Cross Validation ROC AUC: 86.63 %\n"
     ]
    }
   ],
   "source": [
    "mycv = cross_val_score(model, X, y, scoring='roc_auc',cv = 10).mean()\n",
    "print('Decision-Tree 10-Fold Cross Validation ROC AUC:', round(mycv*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gdjQAMNbWFl"
   },
   "source": [
    "In Scikit-Learn, one of the wrapper method techniques is the [SequentialFeatureSelector (SFS)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) function, which allows us to specify if we want to perform forward selection or backward elimination.\n",
    "\n",
    "**Forward selection starts with no features in the model.** In each step, it adds the feature that provides the most significant improvement to the model performance. This process is repeated, adding one feature at a time, until adding new features no longer improves the model performance significantly or a predefined number of features is reached.\n",
    "\n",
    "- Forward selection is useful when you have a large number of features, as it allows you to start building the model gradually. Additionally, it is less computationally intensive.\n",
    "- However, it might miss out on important feature interactions that only become apparent when more features are included in the model.\n",
    "\n",
    "**Backward elimination starts with all the features in the model.** In each step, it removes the least significant feature (the one whose removal causes the least deterioration in the model performance). This process is repeated, removing one feature at a time, until removing more features significantly worsens the model performance or a predefined number of features is reached.\n",
    "\n",
    "- Since backward elimination starts with all features, it considers all feature interactions from the beginning. It can also be more accurate when the number of features is not too large, as it evaluates the model with all possible combinations.\n",
    "- However, it can be computationally intensive, especially if the initial number of features is very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbw8MBo_gRTL"
   },
   "source": [
    "To use the SFS function, we need to specify:\n",
    "\n",
    "- The model that will be used to evaluate the importance of different feature subsets. This will be our sample model.\n",
    "- How many features to select; setting this to `'auto'` will tell the SFS function to automatically determine the ideal number of features.\n",
    "- The direction methodology (forward or backward selection) to be used. In this case, we will use backward elimination as we don't have a significant amount of features.\n",
    "- The scoring metric used to assess the various models being trained. We will use the ROC AUC score to be consistent with our baseline metric.\n",
    "- The number of folds to use in cross-validation for model asssessment (this is an optional but highly recommended parameter). We will use 10-Fold Cross validation to be consistent with our baseline metric.\n",
    "\n",
    "Once we call `.fit()` on the SFS function, it will employ the backward elimination method to identify the most important features as per the specified scoring method and cross-validation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6502,
     "status": "ok",
     "timestamp": 1703823403378,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "TjrYYZXYbw34",
    "outputId": "81e83537-0567-4581-e47d-2806203d3a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature names: ['income', 'married', 'children', 'save_act', 'current_act', 'mortgage']\n"
     ]
    }
   ],
   "source": [
    "sfs = SFS(model, n_features_to_select=None, direction='backward', scoring='roc_auc', cv=10).fit(X,y)\n",
    "\n",
    "# Used to transform the dataset X by selecting only the features that SFS determined to be most important.\n",
    "X_selected = sfs.transform(X)\n",
    "\n",
    "# sfs.get_support() returns a boolean array indicating which features were selected.\n",
    "# By zipping this array with X.columns and iterating through them, the code creates a list of feature names that corresponds to True in the boolean array (selected),\n",
    "  # indicating these features were selected by SFS.\n",
    "selected_feature_names = [name for name, selected in zip(X.columns, sfs.get_support()) if selected]\n",
    "\n",
    "print(\"Selected feature names:\", selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78E4nE0rjH-R"
   },
   "source": [
    "Based on the results, we can then evaluate our model performance after feature selection by only using the subset of features selected by the SFS wrapper method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1703823403378,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "PbWd6Cisjs-s",
    "outputId": "3c900d95-23fe-4cca-e7e6-28408f6053bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score After Wrapping with SFS: 88.0 %\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_selected, y)\n",
    "selected_features_auc = cross_val_score(model, X_selected, y, cv=10, scoring='roc_auc').mean()\n",
    "print(\"ROC AUC Score After Wrapping with SFS:\", round(selected_features_auc,2)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgXy7DHadixz"
   },
   "source": [
    "The SFS wrapper method is not the only methodology we could use. Another method is known as [Recursive Feature Elimination (RFE)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html).\n",
    "\n",
    "RFE is an iterative process to rank features by recursively removing the least important features. It starts by training the model on all available features and then measuring the importance of each feature (usually based on model-specific metrics like feature weights or coefficients).\n",
    "\n",
    "While similar to backward elimination, one key diference is that RFE internally evaluates feature importance versus we are able to specify the performance metric to evaluate feature subsets in SFS (mainly the use-of and number-of folds in cross-validation and a scoring metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1703823634133,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "yu6r6wCPdoJF",
    "outputId": "e80c7c9a-59c9-48e2-fe2a-0de33da71948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature age: Rank 2\n",
      "Feature income: Rank 1\n",
      "Feature married: Rank 1\n",
      "Feature children: Rank 1\n",
      "Feature car: Rank 9\n",
      "Feature save_act: Rank 1\n",
      "Feature current_act: Rank 8\n",
      "Feature mortgage: Rank 1\n",
      "Feature sex_FEMALE: Rank 7\n",
      "Feature region_INNER_CITY: Rank 3\n",
      "Feature region_RURAL: Rank 6\n",
      "Feature region_SUBURBAN: Rank 5\n",
      "Feature region_TOWN: Rank 4\n"
     ]
    }
   ],
   "source": [
    "# Here, let's say we are interested in selecting the five most important features. There is no 'auto' setting for RFE.\n",
    "# RFE will iteratively remove features, retrain the model, and evaluate which features are the most significant until only five features are left.\n",
    "selector_rank = RFE(model, n_features_to_select=5).fit(X, y)\n",
    "\n",
    "# Print the ranking of each feature; RFE still ranks all the features in the dataset even though we only specify five.\n",
    "for i, rank in enumerate(selector_rank.ranking_): # The number of features to select determines how many features will be listed as 'Rank 1'.\n",
    "    print(\"Feature %s: Rank %d\" % (feature_names[i], rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTZ9vlqzxrHP"
   },
   "source": [
    "We can then apply the results from the RFE wrapper method in the same manner as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1703823635299,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "KxvBxiwIxdRo",
    "outputId": "89d1ff25-ccd9-45ed-b245-4b9d6ea8d3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature names: ['income', 'married', 'children', 'save_act', 'mortgage']\n"
     ]
    }
   ],
   "source": [
    "# Select only the features that RFE determined to be most important.\n",
    "X_selected_RFE = selector_rank.transform(X)\n",
    "selected_feature_names = [name for name, selected in zip(X.columns, selector_rank.get_support()) if selected]\n",
    "print(\"Selected feature names:\", selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1703823646564,
     "user": {
      "displayName": "Ethan R Wong",
      "userId": "02960486710277803186"
     },
     "user_tz": 360
    },
    "id": "-AtNnPzTxfY_",
    "outputId": "a567ac32-6491-4036-aab4-8d1e44bfe6f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score After Wrapping with RFE: 88.0 %\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_selected_RFE, y)\n",
    "selected_features_auc_RFE = cross_val_score(model, X_selected_RFE, y, cv=10, scoring='roc_auc').mean()\n",
    "print(\"ROC AUC Score After Wrapping with RFE:\", round(selected_features_auc,2)*100,'%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNP0CP10XR+bM+4b4iMXZuh",
   "collapsed_sections": [
    "ZlFv59qFzRCG",
    "a0RBNiZOzctK",
    "tDJKNOmBzl4Q",
    "6Y5dH79gziEW",
    "eGtRX-PJQZLP"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
